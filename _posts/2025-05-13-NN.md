---
layout: post
title: "Neural Network Notes"
date: 2025-05-13
categories: Control CATEGORY-2
usemathjax: true
---

Artificial Neural Network -- ANN

"Nonlinear Function Approximation"
$$
\hat V (s,\bold w) = \sum^K_{k=1} w_k \sigma (\sum_l w_{kl} i_l (s))
$$
![neuralnet](/_images/neural_net.png)

1 output layer. 1 output unit. 1 input layer. 4 input units. 1 hidden layer. a real-valued weight for each link. feedforward neural network.

A weight corresponds to the efficacy of a synaptic connection. $w$

At each unit, they compute a weighted sum of their input signals, and then apply the result to the activation function, to produce the output. $\sum^K_l w_{kl} i_l(s)$

Activation functions could be: s-shaped, sigmoud,rectifier, step function. $\sigma ()$

Input layer: activations are set to externally-supplied values that go to the inputs of the function that the network is approximating.

"The functions are paramterized by the network's connection weights."

"an ANN with a single hidden layer containing a large enough finite number of sigmoid units can approximate any continuous function on a compact region of the network's input space to any degree of accuracy.(Cybenko, 1989)"

More hidden layers create more abstract understanding of the raw input data. Each **unit** provides a **feature** which contributes to the **understanding**.

Training produces **features** for better **understanding**.

### ANNs learn through stochastic gradent method.

Weights are adjusted in the direction of minimizing a cost-function.

In RL, ANNs use 
- TD errors to learn value functions.
- maximize expected reward like a gradient bandit, or policy gradient algorithm

The **gradient** is a vector of partial derivatives of the weights.

Use **backpropogation** which consists of alternating forward and backward through the network. Roots from this are in optimal control (Yann LeCun)

**Forward Pass**: computes the activation of each unit given the current activations of the network's inputs.

**Backward Pass**: computes the partial derivative of each weight which is the **gradient**.

Common Issues with Backpropogation:
1. backpropogation with too many hidden layers can lead to overfitting.
2. with deep layers learning can be very slow when weights decay rapidly toward the inputs, or very unstable when partial derivates grow rapidly toward the input side.

## References:
1. Reinforcement Learning, Sutton, Barto